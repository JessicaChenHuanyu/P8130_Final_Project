---
title: "**P8130 Final Report (Project 1)**"
author: 
- "Huanyu Chen (hc3451)   Xiaoting Tang (xt2288)"
- "Yifei Liu (yl5508)     Longyu Zhang (lz2951)"
output:
  pdf_document:
    latex_engine: pdflatex
fontsize: 11pt
linestretch: 1.5
---

```{r, include=FALSE}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(tidyr)
library(knitr)
library(MASS)
library(glmnet)
library(leaps)

set.seed(1)
```

# Abstract

(condenses a brief introduction, brief description of methods, and main
results into a one-paragraph summary)

# Introduction

The objective of this study is to use regression models to predict
academic performance in math, reading, and writing based on various
variables, including personal characteristics such as gender, ethnicity,
and parental education, as well as environmental factors like lunch
type, test preparation, and weekly study hours. Furthermore, the study
aims to identify potential correlations and regression model between
scores in different subjects. The combination of these analyses is
intended to provide educators and policymakers with practical insights
for tailoring interventions, improving educational programs, and
building strong support structures that promote students' overall
academic progress.

# Methods

This dataset provides information on public school students, including
three test scores and various personal and socioeconomic factors. To
facilitate analysis, categorical data have been converted to numerical
representations based on their ordinal order or type. We have excluded
the missing cells because they are factorial data types.

After processing the data, we created **Table 1**, which presents a
summary of the factorial data, including the number of missing data, the
number of categories under each variable, and the top counts. For the
numeric data (three test scores), we constructed a comprehensive
descriptive table (**Table 2**) to provide a snapshot of central
tendencies and variability. The distribution of the three response
variables (test scores) is presented in **Figure 1** (histogram) and
**Figure 2** (boxplot), indicating a normal distribution.

Then we fitted the "full model" using the score of three subjects respectively as the response variables, which consists of all 11 categorical variables as predictors. The model diagnostics are conducted by generating four plot for each model: Residuals vs Fitted, Q-Q Residuals, Scale-Location and Residuals vs Leverage (**Figure 3,4,5**). Next, we use BIC-based procedures to select the appropriate subsets of predictors for three subjects (**Figure 6,7,8,9**).

Based on the full models, we did some tests and calculations:

First, we conducted boxcox method (**Figure 10**) to determine if there's any transformation needed.

Second, calculated Cook's distance (**Figure 11**) to check the existence of outliers and influence points.

Finally, in order to test the multicollinearity among predictors, we calculated VIF as the criterion of multicollinearity (**Table 3,4,5**).

After all the steps above, we conducted model selection using both stepwise selection method and LASSO method. For stepwise method, the remaining predictors, coefficients and p-values are reported in **Table 6,7,8**.

In the selection procedure using LASSO method, for each subject we used cross-validation to decide the optimal value of method parameter $\lambda$, and then fitted LASSO model with this optimal value (**Figure 12,13,14**).

Finally, we tried to figure out if it is possible to leverage one score as the auxiliary information to learn the model for another score (still its model against variables 1-11) better. we plotted the correlation among three score variables (**Figure 15**). Then we refitted the linear models for the scores of three subjects using eleven categorical variables and one other score variable of a different subject as predictors (**Table 9,10,11,12,13,14**). The VIFs are calculated for all six models generated in this step to reveal the potential multicollinearity (**Table 15,16,17,18,19,20**).

# Results

**Table 1** gives the summary of the factorial data. **Table 2** provides the mean, deviation and quantile information about the continuous data (score variables of three subjects). The distribution of three response score variables are demonstrated by **Figure 1** (histogram) and **Figure 2** (boxplot). 

**Table 6**, **Table 7**, and **Table 8** display the regression models
for math, reading, and writing scores using both forward and backward
stepwise regression. Moreover, **Figures 3**, **Figures 4**, and
**Figures 5** display the diagnostic plots generated by the model.

**Figure 6** displays the BIC over number of parameters for models of three subjects. **Figure 7,8,9** shows the BIC consistent with the remaining predictors in models of three subjects.

**Figure 10** demonstrates the results of boxcox method: the log-likelihood over boxcox method parameter $\lambda$, and **Figure 11** demonstrates the Cook's distance as the result of testing of outliers.

**Table 3,4,5** display the result of multicollinearity test: VIFs for three full models.

**Figure 12,13,14** are plots of mean cross-validation error over LASSO parameter $\lambda$, providing information about optimal values of $\lambda$ for LASSO models. These optimal values are used for following fitting of LASSO models.

**Figure 15** shows the correlation existing among score variables, suggesting the feasibility of using one of the score variables as the auxiliary information to learn the model for another score. **Table 9,10,11,12,13,14** are the results of regression using one subject score as additional predictor for the prediction of another subject score. **Table 15,16,17,18,19,20** shows the VIFs of these six models, indicating the multicollinearity among predictors.


# Conclusions/Discussion

# Contribution

**Xiaoting Tang**: Method, **Yifei Liu**: Result Display

**Longyu Zhang**: Interpretation, **Huanyu Chen**: Writing

# Appendix

## Table

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Read and Clean Data
data =
  read_csv("./data.csv") |>
  janitor::clean_names() |>
  mutate(
    gender = factor(case_when(
      gender == "male" ~ 0,
      gender == "female" ~ 1,
      )),
    ethnic_group = factor(case_when(
      ethnic_group == "group A" ~ 0,
      ethnic_group == "group B" ~ 1,
      ethnic_group == "group C" ~ 2,
      ethnic_group == "group D" ~ 3,
      ethnic_group == "group E" ~ 4,
      )),
    parent_educ = factor(case_when(
      parent_educ == "some highschool" ~ 0,
      parent_educ == "some college" ~ 1,
      parent_educ == "associate's degree" ~ 2,
      parent_educ == "bachelor's degree" ~ 3,
      parent_educ == "master's degree" ~ 4,
      )),
    lunch_type = factor(case_when(
      lunch_type == "standard" ~ 0,
      lunch_type == "free/reduced" ~ 1,
      )),
    test_prep = factor(case_when(
      test_prep == "none" ~ 0,
      test_prep == "completed" ~ 1,
      )),
    parent_marital_status = factor(case_when(
      parent_marital_status == "married" ~ 0,
      parent_marital_status == "single" ~ 1,
      parent_marital_status == "widowed" ~ 2,
      parent_marital_status == "divorced" ~ 3,
      )),
    practice_sport = factor(case_when(
      practice_sport == "never" ~ 0,
      practice_sport == "sometimes" ~ 1,
      practice_sport == "regularly" ~ 2,
      )),
    is_first_child = factor(case_when(
      is_first_child == "no" ~ 0,
      is_first_child == "yes" ~ 1,
      )),
    transport_means = factor(case_when(
      transport_means == "school_bus" ~ 0,
      transport_means == "private" ~ 1,
      )),
    wkly_study_hours = factor(case_when(
      wkly_study_hours == "< 5" ~ 0,
      wkly_study_hours == "10-May" ~ 1,
      wkly_study_hours == "> 10" ~ 2,
      ))
    ) |>
  mutate(nr_siblings = factor(nr_siblings))

# Another data set for EDA
data_long <- data |>
  pivot_longer(cols = c(math_score, reading_score, writing_score),
               names_to = "test", values_to = "score")
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Summary
sum_data_fct =
  data |>
  dplyr::select(1:11) |>
  skimr::skim() |>
  dplyr::select(skim_variable, n_missing, factor.n_unique, factor.top_counts)

colnames(sum_data_fct) = c("Variable", "Missing", "Unique", "Top Counts")

knitr::kable(x = sum_data_fct, caption = "Categorical Variables pre-analysis", digits = 1)

data =
  data |>
  drop_na()
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
sum_data_score =
  data |>
  dplyr::select(12:14) |>
  skimr::skim() |>
  dplyr::select(skim_variable, numeric.mean, numeric.sd, numeric.p0, numeric.p25, numeric.p50, numeric.p75, numeric.p100)

colnames(sum_data_score) = c("Variable", "Mean", "SD", "Min", "Q1", "Median", "Q3", "Max")

knitr::kable(x = sum_data_score, caption = "Continuous Variables pre-analysis", digits = 1)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
# FULL MODEL
# Math
model_math_full = lm(math_score ~ . - reading_score - writing_score, data = data)

# Reading
model_reading_full = lm(reading_score ~ . - math_score - writing_score, data = data)

# Writing
model_writing_full = lm(writing_score ~ . - reading_score - math_score, data = data)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Stepwise Regressions
# math
math_sr = step(model_math_full, direction = 'both', trace = FALSE)

tb_math_sr = math_sr |>
  broom::tidy() |>
  filter(term != "(Intercept)") |> 
  dplyr::select(term, estimate, p.value)
colnames(tb_math_sr) = c("Term", "Estimate", "P Value")
knitr::kable(x = tb_math_sr, caption = "Math Scores Models by Stepwise Regression", digits = 2)

# reading
rea_sr = step(model_reading_full, direction = 'both', trace = FALSE)

tb_rea_sr = rea_sr |>
  broom::tidy() |>
  filter(term != "(Intercept)") |> 
  dplyr::select(term, estimate, p.value)
colnames(tb_rea_sr) = c("Term", "Estimate", "P Value")
knitr::kable(x = tb_rea_sr, caption = "Reading Scores Models by Stepwise Regression", digits = 2)

# writing
wri_sr = step(model_writing_full, direction = 'both', trace = FALSE)

tb_wri_sr = wri_sr |>
  broom::tidy() |>
  filter(term != "(Intercept)") |> 
  dplyr::select(term, estimate, p.value)
colnames(tb_wri_sr) = c("Term", "Estimate", "P Value")
knitr::kable(x = tb_wri_sr, caption = "Writing Scores Models by Stepwise Regression", digits = 2)
```

## Figure

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Histograms
data_long |>
  ggplot(aes(x = score, fill = test)) +
  geom_histogram(binwidth = 8, color = "#013571") +
  labs(
    title = "Figure 1: Scores Histograms by Subjects",
    x = "Score",
    y = "Count"
    ) +
  scale_fill_manual(values = c("#2E4E7D", "#405165", "#67A9CB")) +
  facet_grid(~ test) +
  theme_bw() +
  theme(legend.position = "bottom") +
  theme(plot.title = element_text(size = 15, face = "bold", hjust = 0.5))
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Boxplots
data_long |>
  ggplot(aes(x = test, y = score, fill = test)) +
  geom_boxplot() +
  labs(
    title = "Figure 2: Scores Boxplot by Subjects",
    x = "Test",
    y = "Score"
    ) +
  facet_wrap(~ test, scales = "free") +
  scale_fill_manual(values = c("#2E4E7D", "#405165", "#67A9CB")) +
  theme_bw() +
  theme(legend.position = "bottom") +
  theme(plot.title = element_text(size = 15, face = "bold", hjust = 0.5))
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
par(mfrow = c(2, 2))
plot(math_sr, which = 1)
plot(math_sr, which = 2)
plot(math_sr, which = 3)
plot(math_sr, which = 4)
main_title <- "Figure 3: Diagnostic Plots for Math Test Score Regression Model"
mtext(main_title, side = 3, line = -2, outer = TRUE)


par(mfrow = c(2, 2))
plot(rea_sr, which = 1)
plot(rea_sr, which = 2)
plot(rea_sr, which = 3)
plot(rea_sr, which = 4)
main_title <- "Figure 4: Diagnostic Plots for Reading Test Score Regression Model"
mtext(main_title, side = 3, line = -2, outer = TRUE)


par(mfrow = c(2, 2))
plot(wri_sr, which = 1)
plot(wri_sr, which = 2)
plot(wri_sr, which = 3)
plot(wri_sr, which = 4)
main_title <- "Figure 5: Diagnostic Plots for Writing Test Score Regression Model"
mtext(main_title, side = 3, line = -2, outer = TRUE)
```
